\documentclass[12pt]{beamer}
\usepackage{natbib}         % Pour la bibliographie
\usepackage{url}            % Pour citer les adresses web
\usepackage[T1]{fontenc}    % Encodage des accents
\usepackage[utf8]{inputenc} % Lui aussi
\usepackage[frenchb]{babel} % Pour la traduction française
\usepackage{numprint}       % Histoire que les chiffres soient bien

\usepackage{amsmath}        % La base pour les maths
\usepackage{mathrsfs}       % Quelques symboles supplémentaires
\usepackage{amssymb}        % encore des symboles.
\usepackage{amsfonts}       % Des fontes, eg pour \mathbb.

\usepackage{multicol}         % Pour pouvoir séparer l'espace en plusieurs colonnes

%\usepackage[svgnames]{xcolor} % De la couleur
\usepackage{geometry}       % Gérer correctement la taille
\usepackage{array}

\usepackage{tikz, pgfplots}
\usetikzlibrary{positioning} % Add this line to load the positioning library
\usetikzlibrary{fit}
\usetikzlibrary{shapes}
\usetikzlibrary{matrix}
\usetikzlibrary{arrows}
\pgfplotsset{compat=1.16}
\usepgfplotslibrary{statistics}

\makeatletter
\def\pgf@matrix@last@nextcell@options{}
\makeatother


\usepackage{listings}
\usepackage{tcolorbox}

\usepackage{comment}

\usetheme{metropolis}

\title{Le traitement du langage naturel par transformers illustré par un exemple pour la classification de texte}

\author{Cerisara Nathan, MPI}
\date{SCEI: 10953}

\begin{document}

\maketitle

%%%%%%%%% PAGE 1 : SOMMAIRE %%%%%%%%%

\begin{frame}{Plan de la présentation}

\begin{enumerate}
  \item Architecture Transformer
  	\begin{enumerate}
  		\item Vectorisation du texte
  		\item La partie Encodeur de l'architecture
  		\item Les matrices d'Attention
  		\item Le réseau Feed Forward
  	\end{enumerate}
  \vspace{7px}
  \item Application personnelle
     \begin{enumerate}
      		\item Objectif
      		\item Le modèle BERT
      		\item La structure du réseau de neurone utilisée
      		\item Les données et l'apprentissage
      		\item Les résultats
     \end{enumerate}
\end{enumerate}

\end{frame}

%%%%%%%%% PAGE 2 : L'architecture Transformer %%%%%%%%%

\begin{frame}{1. L'architecture Transformer}
Schéma de l'architecture dans le cas de la génération :

\begin{center}

\resizebox{!}{(\textheight * 8 / 11)}{%
\begin{tikzpicture}[
Block/.style =  {rectangle,  rounded corners, draw=black!100, fill =white!0, thick, minimum width = 80px},
Eblock/.style = {rectangle, rounded corners, draw=black!40!green, fill =green!10, thick, minimum height = 30px, minimum width = 100px},
Dblock/.style = {rectangle, rounded corners, draw=black!40!blue, fill =blue!10, thick, minimum height = 30px, minimum width = 100px},
Vect/.style = {circle, draw=purple!80!red, thick, font=\footnotesize, text width=8px, align=left},
Fleche1/.style = {->, thick, shorten <=5px, shorten >=5px},
Fleche2/.style= {->,orange, thick, shorten <=5px, shorten >= 5px},
scale=2
]

%Nodes

%Encoder Blocks
\node[Block] (Tok) {Analyseur Lexical};
\node[Block, align=center] (EPE)  [above=15px of Tok] {Représentation Vectorielle + \\Encodage Positionnel};
\node[Eblock] (Enc1) [above=15px of EPE] {Encodeur $1$};
\node[Eblock] (Enc2) [above=15px of Enc1] {Encodeur $2$};
\node (dots1) [above=15px of Enc2] {$\dots$};
\node[Eblock] (EncN) [above=15px of dots1] {Encodeur $N_e$};

%Encoder Input
\node (Einp) [below=20px of Tok] {};
\node[font=\footnotesize] (Einp_dots) [right=0px of Einp] {$\dots$};
\node[Vect] (Einp_2) [left=5px of Einp_dots] {$x_2$};
\node[Vect] (Einp_1) [left=5px of Einp_2] {$x_1$};
\node[Vect] (Einp_N) [right=5px of Einp_dots] {$x_N$};
\node (txt_Einp) [left=5px of Einp_1] {Entrée};

%Encoder Output
\node (Eout) [above=20px of EncN] {};
\node[font=\footnotesize] (Eout_dots) [right=0px of Eout] {$\dots$};
\node[Vect] (Eout_2) [left= 5px of Eout_dots] {$z_2$};
\node[Vect] (Eout_1) [left=5px of Eout_2] {$z_1$};
\node[Vect] (Eout_N) [right=5px of Eout_dots] {$z_N$};
\node (txt_Eout) [left=5px of Eout_1] {Sortie de l'encodeur};

%Decoder Blocks
\node[Dblock] (Dec1) [right=75px of Enc1] {Decodeur $1$};
\node[Dblock] (Dec2) [above=15px of Dec1] {Decodeur $2$};
\node (dots2) [above=15px of Dec2] {$\dots$};
\node[Dblock] (DecN) [above=15px of dots2] {Decodeur $N_d$};
\node[Block, align=center] (Probs) [above=15px of DecN] {Prédit les probabilités\\ du prochain token};

%Decoder Input
\node (Dinp) [below=20px of Dec1] {};
\node[font=\footnotesize] (Dinp_dots) [right=0px of Dinp] {$\dots$};
\node[Vect] (Dinp_2) [left=5px of Dinp_dots] {$y_2$};
\node[Vect] (Dinp_1) [left=5px of Dinp_2] {$y_1$};
\node[Vect] (Dinp_N) [right=5px of Dinp_dots] {$y_N$};
\node (txt_Dinp) [right=5px of Dinp_N] {Entrée Masquée du Décodeur};

%Arrows
\draw[Fleche1] (Einp.north) to (Tok.south);
\draw[Fleche1] (Tok.north) to (EPE.south);
\draw[Fleche1] (EPE.north) to (Enc1.south);
\draw[Fleche1] (Enc1.north) to (Enc2.south);
\draw[Fleche1] (Enc2.north) to (dots1.south);
\draw[Fleche1] (dots1.north) to (EncN.south);
\draw[Fleche1] (EncN.north) to (Eout.south);

\draw[Fleche1] (Dinp.north) to (Dec1.south);
\draw[Fleche1] (Dec1.north) to (Dec2.south);
\draw[Fleche1] (Dec2.north) to (dots2.south);
\draw[Fleche1] (dots2.north) to (DecN.south);
\draw[Fleche1] (DecN.north) to (Probs.south);

\draw[Fleche2] (Eout_N.east) .. controls +(right:25px) and +(left:25px) .. (Dec1.west);
\draw[Fleche2] (Eout_N.east) .. controls +(right:25px) and +(left:25px) .. (Dec2.west);
\draw[Fleche2] (Eout_N.east) .. controls +(right:25px) and +(left:25px) .. (DecN.west);
\end{tikzpicture}
}
\end{center}
\end{frame}


%%%%%%%%% PAGE 3 : Tokenisation & Embeddings %%%%%%%%%

\begin{frame}{1.1 Vectorisation du texte : Analyse Lexicale}

\textbf{Analyseur lexical (bert-base-uncased)}

\textbf{Ex1 : }

\ \textit{PHRASE} : ``Neural Networks are so cool!''\\
\ \textit{TOKENS} : $\begin{array}{ll} [101, 15756, 6125, 2024, 2061, 4658, 999, 102, 0, \dots, 0] \\ \footnotesize \text{[CLS]} \  \text{``neural''} \ \text{``networks''}\  \text{``are''} \ \text{``so''} \ \text{``cool''} \ \text{``!''} \ \text{[SEP]}  \end{array}$

\vspace{15px}

\textbf{Ex2 : }

\ \textit{PHRASE} : ``Bonjour le monde!''\\
\ \textit{TOKENS} : $\begin{array}{ll} [101, 14753, 23099, 2099, 3393, 23117, 999, 102, 0, \dots, 0] \\ \footnotesize \text{[CLS]} \  \text{``bon''} \ \text{``\#\#jou''}\  \text{``\#\#r''} \ \text{``le''} \ \text{``monde''} \ \text{``!''} \ \text{[SEP]}  \end{array}$

\end{frame}


%%%%%%%%% PAGE 4 : Embeddings + Positional Encoding %%%%%%%%%
\begin{frame}{1.1 Vectorisation du texte : Représentation Vectoriel \& Encodage Positionnel}

\begin{center}
\resizebox{!}{(\textheight * 12 / 18)}{
\begin{tikzpicture}[
Fleche1/.style = {->, thick, shorten <=5px, shorten >=5px}
]

%Blocks

\node (Tokens) {Tokens};
\node (MatrixT) [below=5px of Tokens, fill=gray!13] {$T = \begin{bmatrix} t_{1} \\ \vdots \\  t_{N} \end{bmatrix}$};
\node (EmptySpace) [right=80px of Tokens] {};
\node (TokenEmbedding) [above=15px of EmptySpace] {Token Embedding};
\node (EmbeddingE) [above=0px of TokenEmbedding, fill=gray!13] {$E = \begin{bmatrix} e_{1, 1} & \dots & e_{1, d_E} \\ \dots \\  e_{N, 1} & \dots & e_{N, d_E} \end{bmatrix}$};
\node (PositionalEncoding) [below=15px of EmptySpace] {Encodage Positionnel};
\node (PositionalP) [below=0px of PositionalEncoding, fill=gray!13] {$P = \begin{bmatrix} p_{1, 1} & \dots & p_{1, d_E} \\ \dots \\  p_{N, 1} & \dots & p_{N, d_E} \end{bmatrix}$};
\node (LayerNorm) [right=80px of EmptySpace] {LayerNormalization $(E + P)$};
\node (SinusoidalCurves) [right=5px of PositionalP] {};

% Sinusoidal curves
\begin{scope}[xshift=180px, yshift=-40px, yscale=0.08, xscale=0.1, every node/.append style={text=red}]
    \foreach \i in {1,...,5}{
        \draw[domain=0:4*pi, variable=\x, color=red, yshift=-120*\i px] plot ({\x}, {sin(\x*20*\i});
    }
\end{scope}

%Fleches
\draw[Fleche1] (Tokens.east) to (TokenEmbedding.west);
\draw[Fleche1] (Tokens.east) to (PositionalEncoding.west);
\draw[Fleche1] (TokenEmbedding.east) to (LayerNorm.west);
\draw[Fleche1] (PositionalEncoding.east) to (LayerNorm.west);

\end{tikzpicture}
}
\end{center}

\end{frame}


%%%%%%%%% PAGE 5 : Vectorisation du texte %%%%%%%%%

\begin{frame}{1.2 La partie Encodeur}

\begin{center}

\resizebox{!}{(\textheight * 8 / 11)}{%
\begin{tikzpicture}[
EncBlock/.style = {rectangle, rounded corners, draw=green!30, fill=green!5, thick, dashed, minimum width=150px, minimum height=212px},
Block/.style =  {rectangle,  rounded corners, draw=black!100, fill =white!0, thick, minimum width = 80px},
Ablock/.style = {rectangle, rounded corners, draw=black!40!green, fill =green!10, thick, minimum height = 50px, minimum width = 100px},
FFblock/.style = {rectangle, rounded corners, draw=black!40!blue, fill =blue!10, thick, minimum height = 50px, minimum width = 100px},
Vect/.style = {circle, draw=purple!80!red, thick, font=\footnotesize, text width=8px, align=left},
Circ/.style = {circle, draw=black, thick},
Fleche1/.style = {->, thick, shorten <=5px, shorten >= 5px},
]

%Encoder Input
\node (Einp) {};
\node[font=\footnotesize] (Einp_dots) [right=0px of Einp] {$\dots$};
\node[Vect] (Einp_2) [left=5px of Einp_dots] {$x_2$};
\node[Vect] (Einp_1) [left=5px of Einp_2] {$x_1$};
\node[Vect] (Einp_N) [right=5px of Einp_dots] {$x_N$};
\node (txt_Einp) [left=5px of Einp_1] {Entrée};

%Global Encoder Block
\node[EncBlock] (Enc) [above=10px of Einp] {};
\node (Txt_Encoder) [left=8px of Enc] {Bloc Encodeur};

% MultiHead Self-Attention Block
\node[Ablock] (Att) [above=15px of Einp] {Attention Propre};
\node[Block] (AN1) [above=15px of Att] {Somme \& Normalise};

%Attention output
\node (Aout) [above=15px of AN1] {};
\node[font=\footnotesize] (Aout_dots) [right=0px of Aout] {$\dots$};
\node[Vect] (Aout_2) [left=5px of Aout_dots] {$x_2$};
\node[Vect] (Aout_1) [left=5px of Aout_2] {$x_1$};
\node[Vect] (Aout_N) [right=5px of Aout_dots] {$x_N$};

%FF block
\node[FFblock] (FF) [above=15px of Aout] {Couche Feed Forward};
\node[Block] (AN2) [above=15px of FF] {Somme \& Normalise};

%Encoder Output
\node (Eout) [above=15px of AN2] {};
\node[font=\footnotesize] (Eout_dots) [right=0px of Eout] {$\dots$};
\node[Vect] (Eout_2) [left=5px of Eout_dots] {$x_2$};
\node[Vect] (Eout_1) [left=5px of Eout_2] {$x_1$};
\node[Vect] (Eout_N) [right=5px of Eout_dots] {$x_N$};
\node (txt_Eout) [left=5px of Eout_1] {Sortie};

%Arrows
\draw[Fleche1] (Einp.north) to (Att.south);
\draw[Fleche1] (Att.north) to (AN1.south);
\draw[Fleche1] (AN1.north) to (Aout.south);
\draw[Fleche1] (Aout.north) to (FF.south);
\draw[Fleche1] (FF.north) to (AN2.south);
\draw[Fleche1] (AN2.north) to (Eout.south);

\draw[Fleche1] (Einp_N.east) .. controls +(right:45px) and +(right:45px) ..  node[Circ,scale=0.7, fill=white]{$+$}  (AN1.east);
\draw[Fleche1] (Aout_N.east) .. controls +(right:45px) and +(right:45px) .. node[Circ,scale=0.7, fill=white] {$+$} (AN2.east);
\end{tikzpicture}
}
\end{center}
\end{frame}

%%%%%%%%% PAGE 6 : Matrices d'attention %%%%%%%%%

\begin{frame}{1.3 Matrice d'attention}


\begin{center}

\resizebox{(\textwidth * 11 / 11)}{!}{%
\begin{tikzpicture}[
EncBlock/.style = {rectangle, rounded corners, draw=green!30, fill=green!5, thick, dashed, minimum width=150px, minimum height=212px},
Block/.style =  {rectangle,  rounded corners, draw=black!100, fill =white!0, thick, minimum width = 80px},
Ablock/.style = {rectangle, rounded corners, draw=black!40!green, fill =green!10, thick, minimum height = 50px, minimum width = 100px},
FFblock/.style = {rectangle, rounded corners, draw=black!40!blue, fill =blue!10, thick, minimum height = 50px, minimum width = 100px},
Vect/.style = {circle, draw=purple!80!red, thick, font=\footnotesize, text width=8px, align=left},
Circ/.style = {circle, draw=black, thick},
Fleche1/.style = {->, thick, shorten <=5px, shorten >= 5px},
]

\node (X) {$Entree$};
\node (dimX) [below=0px of X] {$(N, d_E)$};
\node[fit=(X) (dimX), draw=black, thick, rounded corners, inner sep=5px] (boxX) {};

\node (K) [right=100px of X] {$K$};
\node (dimK) [below=0px of K] {$(N, d_k)$};
\node[fit=(K) (dimK), draw=black, fill=orange!20!, thick, rounded corners, inner sep=5px] (boxK) {};
\node (K) [right=100px of X] {$K$};
\node (dimK) [below=0px of K] {$(N, d_k)$};

\node (Empty1) [above=25px of K] {};

\node (Q) [above=25px of Empty1] {$Q$};
\node (dimQ) [below=0px of Q] {$(N, d_k)$};
\node[fit=(Q) (dimQ), draw=black, fill=red!20!, thick, rounded corners, inner sep=5px] (boxQ) {};
\node (Q) [above=25px of Empty1] {$Q$};
\node (dimQ) [below=0px of Q] {$(N, d_k)$};

\node (Empty2) [below=25px of K] {};

\node (V) [below=25px of Empty2] {$V$};
\node (dimV) [below=0px of V] {$(N, d_k)$};
\node[fit=(V) (dimV), draw=black, fill=blue!20!, thick, rounded corners, inner sep=5px] (boxV) {};
\node (V) [below=25px of Empty2] {$V$};
\node (dimV) [below=0px of V] {$(N, d_k)$};

\node (softmax) [right=75px of Empty1] {D};
\node (dimD) [below=0px of softmax] {$(N, N)$};
\node[fit=(softmax) (dimD), draw=black, fill=yellow!20!, thick, rounded corners, inner sep=10px] (boxS) {};
\node (dimD) [below=0px of softmax] {$(N, N)$};
\node (softmax) [right=75px of Empty1] {D};

\node (Empty3) [below=10px of softmax] {};

\node (Y) [below right=5px and 75px of boxS] {$Sortie$};
\node (dimY) [below=0px of Y] {$(N, d_E)$};
\node[fit=(Y) (dimY), draw=black, thick, rounded corners, inner sep=5px] (boxY) {};

\node (form) [below right=105px and -35px of Empty3] {\large$\text{Sortie} = \text{softmax} \left ( \frac{QK^\top}{\sqrt{d_k}} \right )V$};

  \node [fit=(form), draw=red, thick, inner sep=5pt, rounded corners] (box) {};


\draw[Fleche1] (boxX.east) to node[left] {$\times W_Q$} (boxQ.west);
\draw[Fleche1] (boxX.east) to node[above] {$\times W_K$} (boxK.west);
\draw[Fleche1] (boxX.east) to node[left] {$\times W_V$} (boxV.west);
\draw[Fleche1] (boxQ.east) to (boxS.west);
\draw[Fleche1] (boxK.east) to (boxS.west);
\draw[Fleche1] (boxS.east) to (boxY.west);
\draw[Fleche1] (boxV.east) to (boxY.west);

\end{tikzpicture}
}
\end{center}

\end{frame}

%%%%%%%%% PAGE 7 : Le réseau Feed Forward %%%%%%%%%

\begin{frame}{1.4 Le réseau Feed Forward}


\setlength{\columnsep}{90pt}
\begin{multicols*}{2}[]

\begin{center}
\resizebox{(\textwidth * 8/ 12)}{!}{%
\begin{tikzpicture}[
Lin/.style = {rectangle, rounded corners, draw=red!30!yellow!50, fill =yellow!10, dashed, thick, minimum height = 15px, minimum width = 150px},
GeLU/.style = {rectangle, rounded corners, draw=blue!30!purple!50, fill =blue!10, dashed, thick, minimum height = 15px, minimum width = 150px},
Vect/.style = {circle, draw=purple!150!red, thick, font=\footnotesize, text width=8px, align=left},
Vect2/.style = {circle, draw=purple!150!red, thick, font=\footnotesize, text width=8px, align=left},
Circ/.style = {circle, draw=black, thick},
Trait/.style = {-, shorten <=1px, shorten >= 1px},
]


%FF Input
\node (FFinp) {};
\node[font=\footnotesize] (FFinp_dots) [right=0px of FFinp] {$\dots$};
\node[Vect] (FFinp_2) [left=10px of FFinp_dots] {$x_2$};
\node[Vect] (FFinp_1) [left=10px of FFinp_2] {$x_1$};
\node[Vect] (FFinp_N) [right=10px of FFinp_dots] {$x_N$};
\node (txt_FFinp) [left=10px of FFinp_1] {Entrée $X$};

%Linear1
\node[Lin] (Lin1) [above=15px of FFinp] {};
\node (Txt_lin1) [left=5px of Lin1] {Couche linéaire 1};


%Hidden Layer
\node (Hl) [above= 15px of Lin1] {};
\node[font=\footnotesize] (Hl_dots) [above= 45px of FFinp] {$\dots$};
\node[Vect] (Hl_2) [left=10px of Hl_dots] {$h_2$};
\node[Vect] (Hl_1) [left=10px of Hl_2] {$h_1$};
\node[Vect2, minimum width=28px] (Hl_D1) [right=10px of Hl_dots] {$\!\!\!\! h_{D-1}$};
\node[Vect] (Hl_D) [right=10px of Hl_D1] {$h_D$};
\node (txt_Hl) [left=10px of Hl_1] {Couche cachée $H$};

%GeLU
\node[GeLU] (Gelu) [above=15px of Hl_dots] {};
\node (Txt_Glu) [left=5px of Gelu] {Activation GeLU};

%Activated Hidden Layer 
\node (AHl) [above= 5px of Gelu] {};
\node[font=\footnotesize] (AHl_dots) [above= 0px of AHl] {$\dots$};
\node[Vect] (AHl_2) [left=10px of AHl_dots] {$h_2$};
\node[Vect] (AHl_1) [left=10px of AHl_2] {$h_1$};
\node[Vect2, minimum width=28px] (AHl_D1) [right=10px of AHl_dots] {$\!\!\!\! h_{D-1}$};
\node[Vect] (AHl_D) [right=10px of AHl_D1] {$h_D$};
\node (txt_AHl) [left=10px of AHl_1] {};

%Linear2
\node[Lin] (Lin2) [above=15px of AHl_dots] {};
\node (Txt_lin2) [left=5px of Lin2] {Couche linéaire 2};

%FF Output
\node (FFout)  [above=15px of Lin2] {};
\node[font=\footnotesize] (FFout_dots) [right=0px of FFout] {$\dots$};
\node[Vect] (FFout_2) [left=10px of FFout_dots] {$y_2$};
\node[Vect] (FFout_1) [left=10px of FFout_2] {$y_1$};
\node[Vect] (FFout_N) [right=10px of FFout_dots] {$y_N$};
\node (txt_FFout) [left=10px of FFout_1] {Sortie $Y$};

%Nodes Connections
\draw[Trait] (FFinp_1.north) to (Hl_D.south);
\draw[Trait] (FFinp_1.north) to (Hl_D1.south);
\draw[Trait] (FFinp_1.north) to (Hl_2.south);
\draw[Trait] (FFinp_1.north) to (Hl_1.south);

\draw[Trait] (FFinp_2.north) to (Hl_D.south);
\draw[Trait] (FFinp_2.north) to (Hl_D1.south);
\draw[Trait] (FFinp_2.north) to (Hl_2.south);
\draw[Trait] (FFinp_2.north) to (Hl_1.south);

\draw[Trait] (FFinp_N.north) to (Hl_D.south);
\draw[Trait] (FFinp_N.north) to (Hl_D1.south);
\draw[Trait] (FFinp_N.north) to (Hl_2.south);
\draw[Trait] (FFinp_N.north) to (Hl_1.south);



\draw[Trait] (AHl_D.south) to (Hl_D.north);
\draw[Trait] (AHl_D1.south) to (Hl_D1.north);
\draw[Trait] (AHl_2.south) to (Hl_2.north);
\draw[Trait] (AHl_1.south) to (Hl_1.north);


\draw[Trait] (FFout_1.south) to (AHl_D.north);
\draw[Trait] (FFout_1.south) to (AHl_D1.north);
\draw[Trait] (FFout_1.south) to (AHl_2.north);
\draw[Trait] (FFout_1.south) to (AHl_1.north);

\draw[Trait] (FFout_2.south) to (AHl_D.north);
\draw[Trait] (FFout_2.south) to (AHl_D1.north);
\draw[Trait] (FFout_2.south) to (AHl_2.north);
\draw[Trait] (FFout_2.south) to (AHl_1.north);

\draw[Trait] (FFout_N.south) to (AHl_D.north);
\draw[Trait] (FFout_N.south) to (AHl_D1.north);
\draw[Trait] (FFout_N.south) to (AHl_2.north);
\draw[Trait] (FFout_N.south) to (AHl_1.north);
\end{tikzpicture}
}
\end{center}

\columnbreak

Couche linéaire : $X \mapsto X\cdot W^\top + B$

\end{multicols*}

\end{frame}


%%%%%%%%% PAGE 8 : Le réseau Feed Forward %%%%%%%%%

\begin{frame}{2.1 Application Personnelle: Objectifs}

\textbf{Objectif :}
\begin{itemize}
	\item Classification de texte
	\item Sentiment : Négatif $\longleftrightarrow$ Positif
	\item Analyser les sentiments des habitants sur différents sujets
	\item Peut aider les mairies à prioritiser certains efforts
\end{itemize}


\end{frame}


%%%%%%%%% PAGE 9 : Le modèle BERT %%%%%%%%%

\begin{frame}{2.2 BERT (Bidirectional Encoder Representations from Transformers)}

\begin{itemize}
	\item Architecture Transformer Particulière (Encodeur seulement)
	\item BERT base : $12 \times$ blocks encoder $\to 112$M paramètres 
	\item BooksCorpus (800M words) et English Wikipedia (2,500M words)
	\item Publié vers fin 2018 par des chercheurs de Google
\end{itemize}

\end{frame}


%%%%%%%%% PAGE 10 : La structure du réseau de neurone utilisée %%%%%%%%%

\begin{frame}{2.3 La structure du réseau de neurone utilisée}


\begin{center}

\resizebox{(\textwidth)}{!}{%
\begin{tikzpicture}[
EncBlock/.style = {rectangle, rounded corners, draw=green!30, fill=green!5, thick, dashed, minimum width=150px, minimum height=212px},
Block/.style =  {rectangle,  rounded corners, draw=black!100, fill =white!0, thick, minimum width = 80px, align=center},
Bblock/.style = {rectangle, rounded corners, draw=black!40!green, fill =green!10, thick, minimum height = 50px, minimum width = 100px, align=center},
FFblock/.style = {rectangle, rounded corners, draw=black!40!blue, fill =blue!10, thick, minimum height = 50px, minimum width = 100px, align=center},
Vect/.style = {ellipse, draw=purple!80!red, thick, text width=8px, align=left, minimum width=40px, minimum height = 30px},
Circ/.style = {circle, draw=black, thick},
Fleche1/.style = {->, thick, shorten <=5px, shorten >= 5px},
Fleche2/.style = {->, thick, shorten <=5px, shorten >= 20px},
]

%Blocks
\node (Inp) {``Entrée''};
\node[Block] (BT) [right=25px of Inp] {Analyseur Lexical\\de BERT};
\node[Bblock] (BE) [right=25px of BT] {Encodeur\\BERT};
\node[FFblock] (FFC) [right=25px of BE] {Classifieur \\ Feed Forward};
\node (Out) [right=50px of FFC] {};
\node[Vect] (Out1) [above=0px of Out] {$ \!\!\! S_{\text{pos}}\quad$};
\node[Vect] (Out2) [below=0px of Out] {$\!\!\! S_{\text{neg}}  \quad$};

%Arrows
\draw[Fleche1] (Inp.east) to (BT.west);
\draw[Fleche1] (BT.east) to (BE.west);
\draw[Fleche1] (BE.east) to (FFC.west);
\draw[Fleche2] (FFC.east) to (Out.west);

\end{tikzpicture}
}
\end{center}



\end{frame}


%%%%%%%%% PAGE 11 : Les données et l'apprentissage %%%%%%%%%

\begin{frame}{2.4 Les données et l'apprentissage}

\begin{itemize}
	\item \textbf{Données : } Twitter Sentiment140  (1.6 millions Tweets)
	\item Entraînement : 50000, Test : 25000
	\item \textbf{Temps d'entraînement : } $\approx$ 5h
	\item \textbf{Algorithme d'apprentissage : } Optimiseur Adam  (extension de la descente de gradient stochastique)
	\item \textbf{Fonction de \textit{Loss} : } \textit{CrossEntropy}
\end{itemize}

\end{frame}



%%%%%%%%% PAGE 12 : Les résultats %%%%%%%%%

\begin{frame}{2.5 Les résultats}

\begin{center}
\resizebox{!}{(\textheight * 5 / 11)}{
\includegraphics{result\_table.png}
}
Twitter news dataset (https://www.kaggle.com)
\end{center}

\end{frame}



%%%%%%%%% PAGE 13 : Conclusion %%%%%%%%%

\begin{frame}{Conclusion}

\begin{itemize}
	\item L'architecture Transformer permet de construire des représentations pertinentes du langage naturel
	\vspace{8px}
	\item Plusieurs de tâches sont possibles : Classification, Génération, Traduction, ...
	\vspace{8px}
	\item Classifieur de texte: analyser les satisfactions des habitants
	\vspace{8px}
	\item Améliorations possibles: Meilleurs données d'entraînement, modèle plus large, ...
\end{itemize}


\end{frame}


%%%%%%%%% PAGE 13 : Annexes %%%%%%%%%

\begin{frame}{Annexes}


\begin{center}
	\resizebox{(\textwidth * 9 / 10)}{!}{%{(\textheight * 4 / 17)}{
	
	\begin{minipage}{(\textwidth * 21  / 20)}
	
		
\setlength{\columnsep}{1pt}
\begin{multicols*}{2}[]

\textbf{Ce que l'on a vu :}

\begin{itemize}
	\item Architecture Transformer
	\begin{itemize}
		\item Le block d'encodeur
		\item Les matrices d'attention
		\item Les réseaux Feed Forward
	\end{itemize}
	\item Application Personnelle
\end{itemize}

\columnbreak

\textbf{Annexes / Ouvertures :}

\begin{itemize}
	\item Optimiseur Adam
	\item La fonction de \textit{loss}
	\item La Normalisation par couche (\textit{LayerNorm})
	\item Fonctions \textit{Softmax} et \textit{GeLU}
	\item La partie décodeur
	\item Comparaison avec le modèle GPT
	\item Bibliographie
	\item Le code python
\end{itemize}

\end{multicols*}
	\end{minipage}
	}
\end{center}



\end{frame}


%%%%%%%%% PAGE 15 : Annexe - L'algorithme de backpropagation %%%%%%%%%

\begin{frame}{Annexes: Adam Optimizer}

\begin{center}
	\resizebox{(\textwidth * 23 / 21)}{!}{%{(\textheight * 4 / 17)}{
	
	\begin{minipage}{(\textwidth * 26  / 20)}
		
		\footnotesize
		\setlength{\columnsep}{25px}
		\begin{multicols*}{2}[]

		\textbf{Descente de gradient stochastique}\\

		Différences avec Descente de Gradient :
		\begin{itemize}
			\item Batch de données \textbf{ vs } Dataset entier
			\item Plus efficace en terme de calculs
			\item Ne se bloque pas forcément dans un minimum local
			\item Plus flexible pour le taux d'apprentissage
		\end{itemize}

		\columnbreak

		\textbf{Adam : Adaptive Moment Estimation}

		\begin{itemize}
			\item pour chaque itération : Moyennes pondérées du gradient sur l'historique récent
			\item Taux d'apprentissage adaptatif (individuellement pour chaque paramètre)
			\item Correction de biais
			\item Mise à jour des paramètres
		\end{itemize}

		\end{multicols*}
		\normalsize
	\end{minipage}
	}
\end{center}

\end{frame}




%%%%%%%%% PAGE 16 : Annexe - Fonction de loss %%%%%%%%%

\begin{frame}{Annexes: Fonction de loss}

\textbf{Fonction Cross Entropy}

Soit $C_1, \dots, C_N$  $N$ classes . Soit $x = (x_1, \dots, x_N)$ la sortie du modèle.
Soit $v$ l'indice de la vraie classe.

pour chaque $i \in [\![1, n]\!]$, $$E_i = - \log \left ( \frac{e^{x_v}}{\sum_{k=1}^N e^{x_k}} \right )$$

Donc au final $CE(x, v) = \frac{1}{N} \sum_{i=1}^N E_i$ 


\end{frame}




%%%%%%%%% PAGE 17 : Annexe - LayerNormalization %%%%%%%%%

\begin{frame}{Annexes: LayerNormalization}

\footnotesize
\textbf{Entrée : } Tensor $X$ de dimensions : (batch\_size, sequence\_length, embedding\_size)

\textbf{Opérations : }

\begin{itemize}
	\item La moyenne $E(X)$ et la Variance $V(X)$ (sur la dimension de l'embedding de l'entrée)
	\item Normalisation : $$N_X = \frac{X - E(X)}{\sqrt{V(X)}}$$
	\item Mise à l'échelle et décalage : $$N_x \cdot \alpha + \beta$$
\end{itemize}

$$\text{LayerNorm(X)} = \frac{X - E(X)}{\sqrt{V(X)}+\varepsilon} \cdot \alpha + \beta$$

\normalsize

\end{frame}


%%%%%%%%% PAGE 18 : Annexe - Fonction Softmax et GeLU %%%%%%%%%

\begin{frame}{Annexes: Fonction Softmax et GeLU}

\begin{center}
	\resizebox{(\textwidth * 10 / 10)}{!}{%{(\textheight * 4 / 17)}{
	
	\begin{minipage}{(\textwidth * 24  / 20)}
		
		\footnotesize
		\setlength{\columnsep}{5px}
		\begin{multicols*}{2}[]

		\textbf{Fonction Softmax:}\\

		Pour $z = (z_1, \dots, z_N)$
		$$\text{Softmax}(z) = \left ( \frac{e^{z_i}}{\sum_{k=1}^{N} e^{z_k}} \right )_i$$
		Pour $i \in [\![1, N]\!]$
		\vspace{70px}

		\columnbreak

		\textbf{ReLU et GeLU :}
		
		\vspace{10px}
		
		\resizebox{\textwidth * 6/ 10}{!}{
		
		\begin{minipage}{(\textwidth * 15  / 20)}
		\begin{figure}
			\centering
				\begin{tikzpicture}
				\begin{axis}[
				   xlabel={$x$},
				   ylabel={$y$},
				   axis lines=middle,
				   xmin=-5, xmax=5,
				   ymin=-1, ymax=5,
				   xtick={-5,...,5},
				   ytick={-1,...,5},
				   grid=both,
				   grid style={line width=0.1pt, draw=gray!30},
				   major grid style={line width=0.2pt,draw=gray!50},
				   minor tick num=1,
				   enlargelimits={abs=0.5},
				   width=10cm,
				   height=8cm,
				]
				
				\addplot[domain=-5:0, samples=100, thick, blue]{0};
				\addplot[domain=0:5, samples=100, thick, blue]{x};
				
				\addplot[domain=-5:5, samples=100, ultra thick, red, dashed]{x/2*(1+tanh(sqrt(2/pi)*(x+0.044715*x^3)))};
				
				\end{axis}
				\end{tikzpicture}
				\caption{Fonctions ReLU et GeLU}
		\end{figure}
		\end{minipage}
		}


		\end{multicols*}
		\normalsize
	\end{minipage}
	}
\end{center}

\end{frame}


%%%%%%%%% PAGE 19 : Annexe - Partie décodeur de l'architecture transformer %%%%%%%%%

\begin{frame}{Annexes: Partie décodeur de l'architecture transformer}


\begin{center}

\resizebox{!}{(\textheight * 9 / 11)}{%
\begin{tikzpicture}[
DecBlock/.style = {rectangle, rounded corners, draw=blue!30, fill=blue!5, thick, dashed, minimum width=160px, minimum height=335px},
Block/.style =  {rectangle,  rounded corners, draw=black!100, fill =white!0, thick, minimum width = 80px},
Ablock/.style = {rectangle, rounded corners, draw=black!40!blue, fill =blue!10, thick, minimum height = 50px, minimum width = 100px},
FFblock/.style = {rectangle, rounded corners, draw=black!40!orange, fill =orange!10, thick, minimum height = 50px, minimum width = 100px},
Vect/.style = {circle, draw=purple!80!red, thick, font=\footnotesize, text width=8px, align=left},
Circ/.style = {circle, draw=black, thick},
Fleche1/.style = {->, thick, shorten <=5px, shorten >= 5px},
]

%Decoder Input
\node (Dinp) {};
\node[font=\footnotesize] (Dinp_dots) [right=0px of Dinp] {$\dots$};
\node[Vect] (Dinp_2) [left=5px of Dinp_dots] {$x_2$};
\node[Vect] (Dinp_1) [left=5px of Dinp_2] {$x_1$};
\node[Vect] (Dinp_N) [right=5px of Dinp_dots] {$x_N$};
\node (txt_Dinp) [left=5px of Dinp_1] {Entrée};

%Global Encoder Block
\node[DecBlock] (Dec) [above=10px of Dinp] {};
\node (Txt_Decoder) [right=22px of Dec] {Bloc Decodeur};

% MultiHead Self-Attention Block
\node[Ablock] (Att) [above=15px of Einp] {Attention Propre Masquée};
\node[Block] (AN1) [above=15px of Att] {Somme \& Normalise};

%Attention output
\node (Aout) [above=15px of AN1] {};
\node[font=\footnotesize] (Aout_dots) [right=0px of Aout] {$\dots$};
\node[Vect] (Aout_2) [left=5px of Aout_dots] {$x_2$};
\node[Vect] (Aout_1) [left=5px of Aout_2] {$x_1$};
\node[Vect] (Aout_N) [right=5px of Aout_dots] {$x_N$};


% MultiHead Cross-Attention Block
\node[Ablock] (Catt) [above=15px of Aout] {Attention Croisée};
\node[Block] (AN2) [above=15px of Catt] {Somme \& Normalise};

% Encoder Output
\node (Eout) [left = 80px of Catt] {};
\node[font=\footnotesize] (Eout_dots) [above=0px of Eout] {$\dots$};
\node[Vect] (Eout_2) [below=5px of Eout_dots] {$y_2$};
\node[Vect] (Eout_1) [below=5px of Eout_2] {$y_1$};
\node[Vect] (Eout_N) [above=5px of Eout_dots] {$y_N$};

\node (Eout_text) [below=7px of Eout_1] {Sortie de l'Encodeur};

\node [fit=(Eout_1) (Eout_N), draw=green, dashed, thick, inner sep=5pt, rounded corners] (box) {};
  
  
%Cross-Attention output
\node (CAout) [above=15px of AN2] {};
\node[font=\footnotesize] (CAout_dots) [right=0px of CAout] {$\dots$};
\node[Vect] (CAout_2) [left=5px of CAout_dots] {$x_2$};
\node[Vect] (CAout_1) [left=5px of CAout_2] {$x_1$};
\node[Vect] (CAout_N) [right=5px of CAout_dots] {$x_N$};


%FF block
\node[FFblock] (FF) [above=15px of CAout] {Couche Feed Forward};
\node[Block] (AN3) [above=15px of FF] {Somme \& Normalise};

%Decoder Output
\node (Dout) [above=15px of AN3] {};
\node[font=\footnotesize] (Dout_dots) [right=0px of Dout] {$\dots$};
\node[Vect] (Dout_2) [left=5px of Dout_dots] {$x_2$};
\node[Vect] (Dout_1) [left=5px of Dout_2] {$x_1$};
\node[Vect] (Dout_N) [right=5px of Dout_dots] {$x_N$};
\node (txt_Dout) [left=5px of Dout_1] {Sortie};

%Arrows
\draw[Fleche1] (Dinp.north) to (Att.south);
\draw[Fleche1] (Att.north) to (AN1.south);
\draw[Fleche1] (AN1.north) to (Aout.south);
\draw[Fleche1] (Aout.north) to (Catt.south);
\draw[Fleche1] (Catt.north) to (AN2.south);
\draw[Fleche1] (AN2.north) to (CAout.south);
\draw[Fleche1] (Eout.east) to (Catt.west);
\draw[Fleche1] (CAout.north) to (FF.south);
\draw[Fleche1] (FF.north) to (AN3.south);
\draw[Fleche1] (AN3.north) to (Dout.south);

\draw[Fleche1] (Einp_N.east) .. controls +(right:65px) and +(right:65px) ..  node[Circ,scale=0.7, fill=white]{$+$}  (AN1.east);
\draw[Fleche1] (Aout_N.east) .. controls +(right:65px) and +(right:65px) ..  node[Circ,scale=0.7, fill=white]{$+$}  (AN2.east);
\draw[Fleche1] (CAout_N.east) .. controls +(right:65px) and +(right:65px) .. node[Circ,scale=0.7, fill=white] {$+$} (AN3.east);
\end{tikzpicture}
}
\end{center}

\end{frame}



%%%%%%%%% PAGE 20 : Annexe - Comparaison avec le modème GPT %%%%%%%%%

\begin{frame}{Annexes: Comparaison avec le modèle GPT}


\begin{center}
	\resizebox{(\textwidth * 9 / 10)}{!}{%{(\textheight * 4 / 17)}{
	
	\begin{minipage}{(\textwidth * 24  / 20)}
		
		\small
		\setlength{\columnsep}{15px}
		\begin{multicols*}{2}[]

			\textit{GPT} : Generative Pre-trained Transformer\\
			\textit{BERT} : Bidirectional Encoder Representation from Transformers

			\textbf{Architecture:}
			\begin{itemize}
				\item BERT : Encodeur seulement
				\item GPT : Decodeur seulement
			\end{itemize}
			
			\textbf{Contexte:}
			\begin{itemize}
				\item BERT : Bidirectionnel
				\item GPT : A gauche seulement
			\end{itemize}
			
			\textbf{Utilisation:}
			\begin{itemize}
				\item BERT : Classification, Traduction
				\item GPT : Génération de texte
			\end{itemize}
			
			\vspace{50px}

			\columnbreak

						
			\begin{center}

			\resizebox{!}{(\textheight * 10  / 11)}{%
			\begin{tikzpicture}[
				DecBlock/.style = {rectangle, rounded corners, draw=blue!30, fill=blue!5, thick, dashed, minimum width=140px, minimum height=206px},
				Block/.style =  {rectangle,  rounded corners, draw=black!100, fill =white!0, thick, minimum width = 80px},
				Ablock/.style = {rectangle, rounded corners, draw=black!40!blue, fill =blue!10, thick, minimum height = 50px, minimum width = 100px},
				FFblock/.style = {rectangle, rounded corners, draw=black!40!orange, fill =orange!10, thick, minimum height = 50px, minimum width = 100px},
				Vect/.style = {circle, draw=purple!80!red, thick, font=\footnotesize, text width=8px, align=left},
				Circ/.style = {circle, draw=black, thick},
				Fleche1/.style = {->, thick, shorten <=5px, shorten >= 5px},
			]

			%Decoder Input
			\node (Dinp) {};
			\node[font=\footnotesize] (Dinp_dots) [right=0px of Dinp] {$\dots$};
			\node[Vect] (Dinp_2) [left=5px of Dinp_dots] {$x_2$};
			\node[Vect] (Dinp_1) [left=5px of Dinp_2] {$x_1$};
			\node[Vect] (Dinp_N) [right=5px of Dinp_dots] {$x_N$};
			\node (txt_Dinp) [left=5px of Dinp_1] {Entrée};

			%Global Encoder Block
			\node[DecBlock] (Dec) [above=10px of Einp] {};
			\node (Txt_Decoder) [left=8px of Dec] {Bloc Décodeur};

			% MultiHead Self-Attention Block
			\node[Ablock] (Att) [above=15px of Dinp] {Attention Propre Masquée};
			\node[Block] (AN1) [above=15px of Att] {Somme \& Normalise};

			%Attention output
			\node (Aout) [above=15px of AN1] {};
			\node[font=\footnotesize] (Aout_dots) [right=0px of Aout] {$\dots$};
			\node[Vect] (Aout_2) [left=5px of Aout_dots] {$x_2$};
			\node[Vect] (Aout_1) [left=5px of Aout_2] {$x_1$};
			\node[Vect] (Aout_N) [right=5px of Aout_dots] {$x_N$};

			%FF block
			\node[FFblock] (FF) [above=15px of Aout] {Couche Feed Forward};
			\node[Block] (AN2) [above=15px of FF] {Somme \& Normalise};

			%Encoder Output
			\node (Eout) [above=15px of AN2] {};
			\node[font=\footnotesize] (Eout_dots) [right=0px of Eout] {$\dots$};
			\node[Vect] (Eout_2) [left=5px of Eout_dots] {$x_2$};
			\node[Vect] (Eout_1) [left=5px of Eout_2] {$x_1$};
			\node[Vect] (Eout_N) [right=5px of Eout_dots] {$x_N$};
			\node (txt_Eout) [left=5px of Eout_1] {Sortie};

			%Arrows
			\draw[Fleche1] (Einp.north) to (Att.south);
			\draw[Fleche1] (Att.north) to (AN1.south);
			\draw[Fleche1] (AN1.north) to (Aout.south);
			\draw[Fleche1] (Aout.north) to (FF.south);
			\draw[Fleche1] (FF.north) to (AN2.south);
			\draw[Fleche1] (AN2.north) to (Eout.south);

			\draw[Fleche1] (Dinp_N.east) .. controls +(right:60px) and +(right:60px) ..  node[Circ,scale=0.7, fill=white]{$+$}  (AN1.east);
			\draw[Fleche1] (Aout_N.east) .. controls +(right:60px) and +(right:60px) .. node[Circ,scale=0.7, fill=white] {$+$} (AN2.east);
			\end{tikzpicture}
			}
			\end{center}
			

		\end{multicols*}
		\normalsize
	\end{minipage}
	}
\end{center}


\end{frame}



%%%%%%%%% PAGE 21 : Annexe - Bibliographie %%%%%%%%%

\begin{frame}{Annexes: Bibliographie}
\footnotesize
\begin{itemize}
	\item Pytorch documentation
	\item ``Attention is all you need'', Google Research, 2017
	\item ``BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding'', Google AI Language, 2018
	\item ``Improving Language Understanding by Generative Pre-Training'' OpenAI, 2018
	\item ``The Illustrated GPT-2 (Visualizing Transformer Language Models)'' (https://jalammar.github.io/illustrated-gpt2/)
\end{itemize}

\end{frame}


%%%%%%%%% PAGE 14 : Annexe - Python %%%%%%%%%

\begin{frame}[fragile]{Annexes: Code python - Classe Réseau de Neurone}
\footnotesize
Structure très simplifiée d'une classe réseau de neurone en pyton avec pytorch : 
\begin{tcolorbox}[colback=white,boxsep=2mm,arc=1pt,
    auto outer arc,left=1mm,right=1mm,top=1mm,bottom=1mm,boxrule=0.5pt,width=\textwidth]
\begin{lstlisting}[language=python]
import torch.nn as nn

class Net(nn.Module):  
    def __init__(self, dim_in, dim_out):
        super().__init__()
        #
        self.layer = nn.Linear(dim_in, dim_out)

    def forward(self, x):
        return self.layer(x)
\end{lstlisting}
\end{tcolorbox}
\normalsize
\end{frame}



\begin{frame}[fragile]{Annexes: Code python - Boucle d'entraînement}
\vspace{-7px}

\footnotesize
\begin{tcolorbox}[colback=white,boxsep=2mm,arc=1pt,
    auto outer arc,left=1mm,right=1mm,top=1mm,bottom=1mm,boxrule=0.5pt,width=\textwidth]

\begin{minipage}{\textwidth * 1 / 11}
\begin{lstlisting}[language=python]
def train_model(self, epochs):
  dataloader = Dataloader(self.train_dataset,...)
  self.model.train()
  for epoch in range(epochs):
    for batch, data in dataloader:
      label = data['target']
      self.optimizer.zero_grad()
      output = self.model(
        ids= data['ids'],
        mask=data['mask'],
        token_type_ids=data['token_type_ids'])
      label = label.type_as(output)
      loss = self.loss_fn(output,label)
      loss.backward()
      self.optimizer.step()
\end{lstlisting}

\end{minipage}
\end{tcolorbox}
\normalsize
\end{frame}




\end{document}